# ads_sync_dashboard - SBEP Mandate v2.2

**Project-Specific Operating Instructions for AI Agents**

---

## Project Context

**Project Name:** ads_sync_dashboard  
**Primary Language/Stack:** Python 3.12, FastAPI, Celery, Redis, Docker  
**Key Integrations:** ads_sync CLI (subprocess), Redis (message broker), Celery (job queue)  
**Current Phase:** Phase 1 Complete - Production-Ready API with Celery Worker

---

## Quick Start for Agents

### Required Reading (In Order)

1. **This file** (`sds/SBEP-MANDATE.md`) - Project-specific agent instructions
2. **`README.md`** - Complete API documentation, endpoints, architecture
3. **`sds/SBEP-INDEX.yaml`** - Complete documentation inventory
4. **`../ads_sync/README.md`** - CLI tool that this dashboard controls
5. **`../ads_sync/sds/SBEP-MANDATE.md`** - CLI project mandate (sibling project)

### Project-Specific Documentation Locations

- **Main README:** `README.md` - API endpoints, quick start, troubleshooting
- **API Routes:** `api/routes/` - FastAPI endpoint definitions
- **Worker Tasks:** `worker/tasks.py` - Celery task definitions
- **Configuration:** `.env` - Environment variables, `api/config.py` - Settings

---

## Project-Specific Rules

### 1. Technology Stack Awareness

**ads_sync_dashboard uses:**
- Python 3.12 (Poetry for dependency management)
- FastAPI 0.104.0 (async web framework)
- Uvicorn (ASGI server)
- Celery 5.3.0 (distributed task queue)
- Redis 5.0.0 (message broker)
- Docker Compose (Redis container)
- Pydantic 2.5.0 (data validation)
- Windows-compatible (uses Celery `--pool=solo`)

**Critical Dependency Constraint:**
- Celery worker MUST use `--pool=solo` on Windows
- RQ was replaced with Celery for Windows compatibility

**Before making changes:**
- Verify Python 3.12 compatibility
- Check `pyproject.toml` for dependency versions
- Test both API server and Celery worker
- Ensure Redis is running (Docker container)

### 2. Integration-Specific Documentation

**Sibling Project Integration:**

- **ads_sync CLI:** `../ads_sync/`
  - This dashboard executes ads_sync CLI commands via subprocess
  - Uses Poetry to run commands: `poetry run python ads_sync_cli.py {command} {slug}`
  - CLI must be functional for dashboard to work
  - All 30 clients configured in `../ads_sync/configs/clients/*.yaml`

**Redis Integration:**
- Started via Docker Compose: `docker-compose up -d`
- Connection: `localhost:6379` (configurable in `.env`)
- Used as Celery broker and result backend
- Health check: `docker exec -it ads_sync_dashboard-redis-1 redis-cli ping`

### 3. Architecture & Deployment

**Decoupled Architecture Pattern:**

```
┌─────────────┐      ┌───────────┐      ┌──────────────┐
│   Frontend  │─────▶│  FastAPI  │─────▶│    Redis     │
│   (Future)  │      │    API    │      │  Job Queue   │
└─────────────┘      └───────────┘      └──────────────┘
                                               │
                                               ▼
                                        ┌──────────────┐
                                        │   Celery     │
                                        │   Worker     │
                                        │  (executes   │
                                        │   ads_sync   │
                                        │   CLI)       │
                                        └──────────────┘
```

**Key Principles:**
- API never blocks on long-running operations
- All CLI commands execute in background via Celery
- Job status tracked via Celery AsyncResult
- Subprocess output captured (stdout, stderr, exit_code)

**Starting the Stack:**

1. **Redis (Docker):**
   ```powershell
   cd C:\Users\james\Desktop\Projects\ads_sync_dashboard
   docker-compose up -d
   ```

2. **FastAPI Server:**
   ```powershell
   poetry run uvicorn api.main:app --port 8000
   ```

3. **Celery Worker (separate terminal):**
   ```powershell
   poetry run celery -A worker.tasks worker --pool=solo --loglevel=info
   ```

### 4. Testing & Validation

**Before Committing Changes:**
- Test API endpoint with `curl` or Swagger UI (http://localhost:8000/docs)
- Verify Celery worker receives and processes jobs
- Check job status endpoint returns correct state
- Verify subprocess execution captures output correctly
- Test with at least 1 ads_sync CLI command (e.g., `validate`)

**Testing Approach:**
- Manual API testing via Swagger UI or `curl`
- Test job lifecycle: queue → started → finished/failed
- Verify Redis connection in health check
- Test error handling and job failure scenarios

**Health Checks:**
- API: `GET /health` - Should return `{"status":"healthy"}`
- Redis: `docker ps` - Container should be running
- Celery: `poetry run celery -A worker.tasks inspect active` - Should show worker

### 5. Documentation Maintenance

**When Adding Endpoints:**
- Update `README.md` API Endpoints section
- Add Pydantic models to `api/models/requests.py` or `responses.py`
- Document request/response schemas
- Update OpenAPI docs (auto-generated by FastAPI)

**When Modifying Worker:**
- Update `worker/tasks.py` with clear docstrings
- Document expected CLI command format
- Update error handling patterns
- Test subprocess execution thoroughly

---

## Common Tasks Reference

### Task: Add New API Endpoint

1. Create route in `api/routes/{category}.py`
2. Define Pydantic request/response models
3. Import route in `api/main.py`
4. Test via Swagger UI at http://localhost:8000/docs
5. Update README.md API Endpoints section
6. Test error handling

### Task: Add New Celery Task

1. Define task in `worker/tasks.py` with `@celery_app.task` decorator
2. Document expected parameters and return value
3. Test task execution: `task.apply_async(kwargs={...})`
4. Verify result retrieval via `AsyncResult(task_id)`
5. Update README if user-facing

### Task: Debug Failed Job

1. Check Celery worker logs (running terminal)
2. Get job result: `GET /api/jobs/{job_id}/result`
3. Check subprocess stdout/stderr in result
4. Verify ads_sync CLI command works manually:
   ```powershell
   cd ..\ads_sync
   poetry run python ads_sync_cli.py {command} {slug}
   ```
5. Check Poetry executable path resolution in `worker/cli_executor.py`

### Task: Add Enhanced Diagnostics

1. Implement `GET /api/jobs/{job_id}/debug` endpoint
2. Return rich diagnostic info:
   - Task state and metadata
   - Worker name and timestamp
   - Full traceback if failed
   - Original function call
   - Subprocess command and output
3. Add comprehensive logging
4. Test with both successful and failed jobs

### Task: Set Up Automated Scheduling

1. Configure Celery Beat in `worker/tasks.py`:
   ```python
   celery_app.conf.beat_schedule = {
       'daily-append': {
           'task': 'execute_cli_command',
           'schedule': crontab(hour=8, minute=15),  # 8:15 AM daily
           'kwargs': {'command': 'append', 'slug': 'all'}
       }
   }
   ```
2. Start beat scheduler: `poetry run celery -A worker.tasks beat`
3. Update README with scheduling documentation

---

## Escalation Path

**If Documentation is Insufficient:**
1. Check `../ads_sync/` sibling project documentation
2. Review FastAPI docs for framework features
3. Review Celery docs for task queue patterns
4. Check Redis/Docker logs for infrastructure issues
5. Test ads_sync CLI manually to isolate issues
6. THEN ask user with evidence of search performed

**When Asking for Help:**
Provide:
- Files consulted: `README.md`, `worker/tasks.py`, etc.
- Methods attempted: API calls, CLI commands tested
- Error messages: Full output from API, Celery worker, or CLI
- Job state: Job ID, status, result
- Why each approach failed: Specific error analysis

---

## Project-Specific Anti-Patterns

**Avoid:**
- Using RQ instead of Celery (Windows incompatible)
- Running Celery worker without `--pool=solo` on Windows
- Blocking API endpoints with synchronous operations
- Hardcoding ads_sync project path (use `.env`)
- Not checking Redis connection before queuing jobs
- Ignoring subprocess exit codes
- Not capturing CLI stdout/stderr
- Starting Celery beat without beat service
- Modifying job results after task completion

**Prefer:**
- Celery with `--pool=solo` for Windows
- Async FastAPI endpoints (non-blocking)
- Environment variables for configuration
- Subprocess with full output capture
- Proper error handling and logging
- AsyncResult for job status checking
- Docker Compose for Redis management
- Poetry for running ads_sync CLI
- Clear separation of concerns (API vs Worker)

---

## Sibling Project Relationship

**ads_sync CLI (Parent Project):**
- Location: `../ads_sync/`
- Purpose: CLI tool for Google Ads data sync
- Relationship: This dashboard is the API wrapper for the CLI

**Critical Integration Points:**

1. **CLI Execution:**
   - Worker calls CLI via subprocess
   - Uses Poetry to ensure correct environment
   - Captures stdout, stderr, exit_code
   - Located: `worker/cli_executor.py`

2. **Client Configuration:**
   - All clients defined in `../ads_sync/configs/clients/*.yaml`
   - Dashboard reads client list from ads_sync project
   - 30 active clients as of Phase 1

3. **Data Location:**
   - Master data: `../ads_sync/data/{slug}/*.csv`
   - State files: `../ads_sync/state/{slug}.json`
   - Dashboard reports on data managed by CLI

**When Modifying Either Project:**
- Test integration after changes to either side
- Verify subprocess execution still works
- Check that CLI commands return expected output
- Ensure data paths remain consistent

---

## Critical Configuration

**Environment Variables (`.env`):**
```env
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
ADS_SYNC_PATH=../ads_sync
ADS_SYNC_CLI_COMMAND=poetry run python ads_sync_cli.py
API_PORT=8000
CORS_ORIGINS=*
```

**Docker Compose (`docker-compose.yml`):**
- Redis container configuration
- Port mapping: 6379:6379
- Volume: `redis_data`
- Health check configured

---

## Success Metrics

Agent performance is measured by:
- Documentation consulted before implementation
- Proper testing of both API and worker
- Redis connection verified before operations
- Subprocess execution properly handled
- Error handling implemented correctly
- Clear logging and diagnostics
- Integration with ads_sync CLI maintained
- Windows compatibility preserved

---

## Version History

- **v2.2** (2025-10-23): SBEP v2.2 compliance with complete operational framework
- **v2.0** (2025-10-15): SBEP v2.0 compliance, Celery migration complete
- **v1.0** (2025-10-14): Initial FastAPI + Celery setup

---

**Current Status:** Production-Ready, RQ→Celery Migration Complete

**Next Steps:** 
- Implement enhanced diagnostics (`/debug` endpoint)
- Set up Celery Beat for scheduled jobs
- Add comprehensive logging

---

**Remember:** This dashboard is a thin API layer over the ads_sync CLI. The CLI must be functional for the dashboard to work. Always test CLI commands manually before debugging the dashboard.
